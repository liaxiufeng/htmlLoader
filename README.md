## 介绍



网站链接+保存地址=免费获得一份网站源代码

>  这是一个使用**python**实现的爬虫项目，初衷是由于网上有很多好看的html模板，但下载的时候却要付费，就很头疼。
>
> 于是咬咬牙，就手写了一个爬虫程序。
>
> 由于就用了几天，所以并没有使用队列和多线程。
>
> 但经过多次测试，下载一个网站的全部前端文件，包括但不限于**html,css,js,图片、字体文件**等，耗时大概在几秒到几分钟不等，取决于网站文件的多少。



## 运行

**1.提供模板或网站的其中一个页面链接，赋值给 indexUrl，如下代码，位于文件的末尾。**

> 链接被要求能跳转到网站的其他页面，建议用首页
>
> 注意：如果网站的链接，v2版本要求html链接中必须包含.html，这里是一个值得优化的地方，如果你希望我尽快更新，可以给我发消息，也可以使用v1版本。

```
indexUrl = "http://demo.uu2018.com/962/index.html"
```



**2.提供一个文件保存的位置，赋值给fileHome，如下代码，位于文件的末尾。**

> 程序会检查路径是否存在，并构建出完整路径，但建议不要包含特殊字符或中文，没试过，不知道会不会出错

```
fileHome = "D:/desktop/runtime/pythonLoader/txt/editor2"
```



**3.运行代码**

> 该程序需要python解释器才能运行，并且需要你环境中有对应的依赖
>
> 如requests,etree



## 运行失败

如果程序下载失败，可能是以下原因

* 文件路径带有特殊字符或中文
* html网址不包含.html（程序无法识别文件类型）
* 网站需要带有cookie的某些值才能访问
* 



## 版本

> v1:
>
> 使用识别到的链接都会访问一次的原则，对能识别的链接来者不拒，从链接中得到文件名和保存路径。
>
> v2:
>
> 在v1的基础上，对链接的文件类型做出更好的识别，不同的文件内容有不同的处理方式，对二进制文件更加友好，需要更少的请求次数，和更完整的日志信息。



### 共同点

这里一共上传了2个版本，俩个版本的运行结果几乎差不多。



### 不同点

* 实现原理不同：

* v1版本的兼容性更好，建议v2版本不好用时使用v1。

* v2版本的理论运行时间更短，扩展性更好，发送请求的次数更少，被封ip的可能性更小



## 其他

* 程序并不是很完善，不是主学python。

* 如果代码让你觉得不规范，那我只能说声抱歉。

* 如果对你有帮助，希望更新版本，修复bug，记得给我发消息。

* 如果你愿意，也可以关注关注。

